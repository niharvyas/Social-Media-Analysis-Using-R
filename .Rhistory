library(igraph)
library(httpuv)
api_key <- "AIzaSyDaHsyaod4SETXVBfMqzF8VDQrOaGknSWM"
client_id <- "822547161727-211r7sihciepum6oavf1ap973m0pbnej.apps.googleusercontent.com"
client_secret <- "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"
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
yt_oauth(app_id = client_id, app_secret = client_secret)
yt_oauth(app_id = client_id, app_secret = client_secret)
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
View(video_search)
get_stats(video_id = "6nhEupPe1_M")
video_ids <- as.vector(video_search$video_id[1:3])
yt_data <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
video_ids <- as.vector(video_search$video_id[1:5])
yt_data <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
View(yt_data)
yt_actor_network <- yt_data %>% Create("actor")
yt_actor_graph <- Graph(yt_actor_network)
undir_yt_actor_graph <- as.undirected(yt_actor_graph, mode = "collapse")
louvain_yt_actor <- cluster_louvain(undir_yt_actor_graph)
sizes(louvain_yt_actor)
plot(louvain_yt_actor,
undir_yt_actor_graph,
vertex.label = V(undir_yt_actor_graph)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
write.graph(undir_yt_actor_graph, file = "undir_yt_actor_graph.graphml", format = "graphml")
eb_yt_actor <- cluster_edge_betweenness(undir_yt_actor_graph)
sizes(eb_yt_actor)
plot(eb_yt_actor,
undir_yt_actor_graph,
vertex.label = V(undir_yt_actor_graph)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
yt_actor_graph2 <- yt_actor_graph
V(yt_actor_graph2)$name <- V(yt_actor_graph2)$screen_name
undir_yt_actor_graph2 <- as.undirected(yt_actor_graph2, mode = "collapse")
eb_yt_actor2 <- cluster_edge_betweenness(undir_yt_actor_graph2)
is_hierarchical(eb_yt_actor2)
as.dendrogram(eb_yt_actor2)
plot_dendrogram(eb_yt_actor2)
plot_dendrogram(eb_yt_actor2, mode = "dendrogram", xlim = c(1,20))
View(yt_data)
load("C:/Users/nihar/OneDrive/Desktop/Griffith University/Trimester-1/BDA and SM/Assignment/my8000tweets.RData")
library(vosonSML)
library(magrittr)
library(tidyr)
library(tidytext)
library(stopwords)
library(textclean)
library(qdapRegex)
library(tm)
library(SnowballC)
library(ggplot2)
clean_text <- twitter_data$tweets$text %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_emoji() %>%
replace_emoticon()
Dtext_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
Dtext_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
text_corpus <- text_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords(kind = "SMART")) %>%
tm_map(stemDocument) %>%
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
dtm_df <- as.data.frame(as.matrix(doc_term_matrix))
View(dtm_df)
freq <- sort(colSums(dtm_df), decreasing = TRUE)
head(freq, n = 10)
word_frequ_df <- data.frame(word = names(freq), freq)
View(word_frequ_df)
ggplot(subset(word_frequ_df, freq > 2), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Word Frequency") +
xlab("Words") +
ylab("Frequency")
ggplot(subset(word_frequ_df, freq > 67), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
ggtitle("Word Frequency") +
xlab("Words") +
ylab("Frequency")
library(vosonSML)
library(magrittr)
library(tidytext)
library(igraph)
twomode_network <- twitter_data %>% Create("twomode",
removeTermsOrHashtags = c("#ArmaanMalik OR #Armaanians OR #TeamArmaan OR #Armaaniansarmy"))
twomode_graph <- twomode_network %>% Graph()
write.graph(twomode_graph, file = "TwitterTwomode.graphml", format = "graphml")
length(V(twomode_graph))
V(twomode_graph)$name
twomode_comps <- components(twomode_graph, mode = c("weak"))
twomode_comps$no
twomode_comps$csize
head(twomode_comps$membership, n = 30)
largest_comp <- which.max(twomode_comps$csize)
twomode_subgraph <- twomode_graph %>%
induced_subgraph(vids = which(twomode_comps$membership == largest_comp))
sort(degree(twomode_subgraph, mode = "in"), decreasing = TRUE)[1:20]
sort(degree(twomode_subgraph, mode = "out"), decreasing = TRUE)[1:20]
sort(degree(twomode_subgraph, mode = "total"), decreasing = TRUE)[1:20]
sort(closeness(twomode_subgraph, mode = "in"), decreasing = TRUE)[1:20]
sort(closeness(twomode_subgraph, mode = "out"), decreasing = TRUE)[1:20]
sort(closeness(twomode_subgraph, mode = "total"), decreasing = TRUE)[1:20]
sort(betweenness(twomode_subgraph, directed = FALSE), decreasing = TRUE)[1:20]
load("C:/Users/nihar/OneDrive/Desktop/Griffith University/Trimester-1/BDA and SM/Assignment/my8000tweets.RData")
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
sentiment_scores <- get_sentiment(clean_text, method = "afinn") %>% sign()
library(vosonSML)
library(magrittr)
library(tidytext)
library(textclean)
library(qdapRegex)
library(syuzhet)
library(ggplot2)
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
sentiment_scores <- get_sentiment(clean_text, method = "afinn") %>% sign()
sentiment_df <- data.frame(text = clean_text, sentiment = sentiment_scores)
View(sentiment_df)
sentiment_df$sentiment <- factor(sentiment_df$sentiment, levels = c(1, 0, -1),
labels = c("Positive", "Neutral", "Negative"))
View(sentiment_df)
ggplot(sentiment_df, aes(x = sentiment)) +
geom_bar(aes(fill = sentiment)) +
scale_fill_brewer(palette = "RdGy") +
labs(fill = "Sentiment") +
labs(x = "Sentiment Categories", y = "Number of Tweets") +
ggtitle("Sentiment Analysis of Tweets")
emo_scores <- get_nrc_sentiment(clean_text)[ , 1:8]
emo_scores_df <- data.frame(clean_text, emo_scores)
View(emo_scores_df)
emo_sums <- emo_scores_df[,2:9] %>%
sign() %>%
colSums() %>%
sort(decreasing = TRUE) %>%
data.frame() / nrow(emo_scores_df)
names(emo_sums)[1] <- "Proportion"
View(emo_sums)
ggplot(emo_sums, aes(x = reorder(rownames(emo_sums), Proportion),
y = Proportion,
fill = rownames(emo_sums))) +
geom_col() +
coord_flip()+
guides(fill = "none") +
scale_fill_brewer(palette = "Dark2") +
labs(x = "Emotion Categories", y = "Proportion of Tweets") +
ggtitle("Emotion Analysis of Tweets")
View(emo_scores_df)
View(emo_sums)
json_data <- toJSON(emo_scores)
write(json_data, "ArmaanMalikemoscores.json")
library(vosonSML)
library(magrittr)
library(tidytext)
library(textclean)
library(qdapRegex)
library(syuzhet)
library(ggplot2)
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
sentiment_scores <- get_sentiment(clean_text, method = "afinn") %>% sign()
sentiment_df <- data.frame(text = clean_text, sentiment = sentiment_scores)
sentiment_df$sentiment <- factor(sentiment_df$sentiment, levels = c(1, 0, -1),
labels = c("Positive", "Neutral", "Negative"))
View(sentiment_df)
json_data <- toJSON(sentiment_df$sentiment)
write(json_data, "ArmaanMalikSenti.json")
app_id <- "2a65a39e2a32428abb4b11f7583e6047"
app_secret <- "9680d4b2bef244f0b1ff72f7ebb9c5c2"
token <- "1"
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
armaan_features <- get_artist_audio_features("Armaan Malik")
View(armaan_features)
library(spotifyr)
library(C50)
library(caret)
library(e1071)
library(dplyr)
app_id <- "2a65a39e2a32428abb4b11f7583e6047"
app_secret <- "9680d4b2bef244f0b1ff72f7ebb9c5c2"
token <- "1"
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
armaan_features <- get_artist_audio_features("Armaan Malik")
data.frame(colnames(armaan_features))
armaan_features_subset <- drake_features[ , 9:20]
armaan_features_subset <- armaan_features[ , 9:20]
View(armaan_features_subset)
top100_features <- get_playlist_audio_features("spotify", "37i9dQZF1DX7I09vdUzaSD")
View(top100_features)
data.frame(colnames(top100_features))
top100_features_subset <- top100_features[ , 6:17]
View(top100_features_subset)
top100_features_subset <- top100_features_subset %>% rename(track_id = track.id)
top100_features_noarmaan <- anti_join(top100_features_subset,
armaan_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noarmaan, armaan_features_subset)
comb_data$isArmaanMalik <- factor(comb_data$isArmaanMalik)
top100_features_noarmaanmalik <- anti_join(top100_features_subset,
armaan_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noarmaanmalik, armaan_features_subset)
comb_data$isArmaanMalik <- factor(comb_data$isArmaanMalik)
comb_data <- select(comb_data, -track_id)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
dt_model <- train(isArmaanMalik~ ., data = training_set, method = "C5.0")
install.packages('C50')
install.packages("C50")
library(spotifyr)
library(C50)
library(caret)
library(e1071)
library(dplyr)
app_id <- "2a65a39e2a32428abb4b11f7583e6047"
app_secret <- "9680d4b2bef244f0b1ff72f7ebb9c5c2"
token <- "1"
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
armaan_features <- get_artist_audio_features("Armaan Malik")
data.frame(colnames(armaan_features))
armaan_features_subset <- armaan_features[ , 9:20]
top100_features <- get_playlist_audio_features("spotify", "37i9dQZF1DX7I09vdUzaSD")
data.frame(colnames(top100_features))
top100_features_subset <- top100_features[ , 6:17]
top100_features_subset <- top100_features_subset %>% rename(track_id = track.id)
top100_features_subset["isArmaanMalik"] <- 0
armaan_features_subset["isArmaanMalik"] <- 1
top100_features_noarmaanmalik <- anti_join(top100_features_subset,
armaan_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noarmaanmalik, armaan_features_subset)
comb_data$isArmaanMalik <- factor(comb_data$isArmaanMalik)
comb_data <- select(comb_data, -track_id)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
dt_model <- train(isArmaanMalik~ ., data = training_set, method = "C5.0")
prediction_row <- 1 # MUST be smaller than or equal to training set size
if (tibble(predict(dt_model, testing_set[prediction_row, ])) ==
testing_set[prediction_row, 12]){
print("Prediction is correct!")
} else {
("Prediction is wrong")
}
confusionMatrix(dt_model, reference = testing_set$isArmaanMalik)
load("C:/Users/nihar/OneDrive/Desktop/Griffith University/Trimester-1/BDA and SM/Assignment/my8000tweets.RData")
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
library(vosonSML)
library(magrittr)
library(tidytext)
library(textclean)
library(qdapRegex)
library(tm)
library(topicmodels)
library(slam)
library(Rmpfr)
library(dplyr)
library(ggplot2)
library(reshape2)
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
text_corpus <- text_corpus %>%
tm_map(removeWords, stopwords(kind = "SMART"))
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix_5_1.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix_5_1.RData")
lda_model <- LDA(dtm, k = 6)
tweet_topics <- tidy(lda_model, matrix = "beta")
top_terms <- tweet_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
json_data <- toJSON(tweet_topics)
library(jsonlite)
json_data <- toJSON(tweet_topics)
write(json_data, "ArmaanMalikTweet_topic.json")
library(tuber)
library(vosonSML)
library(magrittr)
library(igraph)
library(httpuv)
library(jsonlite)
api_key <- "AIzaSyDaHsyaod4SETXVBfMqzF8VDQrOaGknSWM"
client_id <- "822547161727-211r7sihciepum6oavf1ap973m0pbnej.apps.googleusercontent.com"
client_secret <- "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"
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("Armaan Malik")
View(video_search)
get_stats(video_id = "6nhEupPe1_M")
video_ids <- as.vector(video_search$video_id[1:5])
yt_data <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
json_data <- toJSON(yt_data)
write(json_data, "ArmaanMalikyt.json")
yt_data <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
yt_actor_network <- yt_data %>% Create("actor")
yt_actor_graph <- Graph(yt_actor_network)
undir_yt_actor_graph <- as.undirected(yt_actor_graph, mode = "collapse")
louvain_yt_actor <- cluster_louvain(undir_yt_actor_graph)
json_data <- toJSON(louvain_yt_actor)
library(vosonSML)
library(magrittr)
library(tidyr)
library(tidytext)
library(stopwords)
library(textclean)
library(qdapRegex)
library(tm)
library(SnowballC)
library(ggplot2)
library(jsonlite)
load("C:/Users/nihar/OneDrive/Desktop/Griffith University/Trimester-1/BDA and SM/Assignment/my8000tweets.RData")
clean_text <- twitter_data$tweets$text %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_emoji() %>%
replace_emoticon()
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
text_corpus <- text_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords(kind = "SMART")) %>%
tm_map(stemDocument) %>%
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
dtm_df <- as.data.frame(as.matrix(doc_term_matrix))
freq <- sort(colSums(dtm_df), decreasing = TRUE)
json_data <- toJSON(freq)
write(json_data, "ArmaanMalikfreq.json")
dtm_df <- as.data.frame(as.matrix(doc_term_matrix))
json_data <- toJSON(dtm_df)
write(json_data, "ArmaanMalikfreq.json")
doc_term_matrix <- DocumentTermMatrix(text_corpus)
json_data <- toJSON(doc_term_matrix)
library(vosonSML)
library(magrittr)
library(tidyr)
library(tidytext)
library(stopwords)
library(textclean)
library(qdapRegex)
library(tm)
library(SnowballC)
library(ggplot2)
library(jsonlite)
clean_text <- twitter_data$tweets$text %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_emoji() %>%
replace_emoticon()
text_corpus <- VCorpus(VectorSource(clean_text))
text_corpus[[1]]$content
text_corpus[[5]]$content
text_corpus <- text_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords(kind = "SMART")) %>%
tm_map(stemDocument) %>%
tm_map(stripWhitespace)
text_corpus[[1]]$content
text_corpus[[5]]$content
doc_term_matrix <- DocumentTermMatrix(text_corpus)
json_data <- toJSON(doc_term_matrix)
dtm_df <- as.data.frame(as.matrix(doc_term_matrix))
View(dtm_df)
freq <- sort(colSums(dtm_df), decreasing = TRUE)
word_frequ_df <- data.frame(word = names(freq), freq)
json_data <- toJSON(word_frequ_df)
write(json_data, "ArmaanMalikfreq.json")
save.image("C:/Users/nihar/OneDrive/Desktop/Griffith University/Trimester-1/BDA and SM/Assignment/Milestone-2/Milestone_2.RData")
