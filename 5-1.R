# k-means Clustering ------------------------------------------------------

# Load packages required for this part

library(rtweet)
library(ggplot2)


View(user_data)


# Create a scatter plot of friends count vs followers count

ggplot(user_data, aes(x = followers_count, y = friends_count)) + 
  geom_point(stat = "identity") + 
  xlab("Followers Count") + 
  ylab("Friends Count") + 
  ggtitle("Friends vs Followers")


# Recreate the same plot using logarithmic scales

ggplot(user_data, aes(x = log(followers_count), y = log(friends_count))) + 
  geom_point(stat = "identity") + 
  xlab("Log Followers Count") + 
  ylab("Log Friends Count") + 
  ggtitle("Friends vs Followers - Log Scale")


# Convert Friends Count and Followers Count via the logarithmic function

log_friends_count <- log(user_data$friends_count)
log_followers_count <- log(user_data$followers_count)


# Create data frame with the log data derived above

user_data_log <- data.frame(user_data$name, log_followers_count, log_friends_count)
View(user_data_log)


# Remove invalid records from the data frame

user_data_log <- subset(user_data_log, user_data_log$log_friends_count != "-Inf")
user_data_log <- subset(user_data_log, user_data_log$log_followers_count != "-Inf")


# Run the k-means clustering algorithm
# and view the result

user_clusters <- kmeans(user_data_log[ , 2:ncol(user_data_log)],
                        centers = 5,
                        iter.max = 10,
                        nstart = 10)

user_data_log$cluster <- factor(user_clusters$cluster)
View(user_data_log)


# Plot the clusters

ggplot(user_data_log, aes(x = log_followers_count, 
                          y = log_friends_count,
                          colour = cluster)) +
  geom_point(stat = "identity") + 
  xlab("Log Followers Count") + 
  ylab("Log Friends Count") + 
  ggtitle("Friends vs Followers - Log Scale")


# View the cluster centres

user_clusters$centers



# Topic Modelling ---------------------------------------------------------

# Load packages required for this part

library(vosonSML)
library(magrittr)
library(tidytext)
library(textclean)
library(qdapRegex)
library(tm)
library(topicmodels)
library(slam)
library(Rmpfr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(jsonlite)

# Set up Twitter authentication variables

my_app_name <- "XXX"
my_api_key <- "XXX"
my_api_secret <- "XXX"
my_access_token <- "XXX"
my_access_token_secret <- "XXX"


# Authenticate to Twitter and collect data
# Refer to the instructions if your Twitter access has been suspended

twitter_data <- Authenticate("twitter",
                             appName = my_app_name,
                             apiKey = my_api_key,
                             apiSecret = my_api_secret,
                             accessToken = my_access_token,
                             accessTokenSecret = my_access_token_secret) %>%
  Collect(searchTerm = "#auspol",
          searchType = "recent",
          numTweets = 1000,
          lang = "en",
          includeRetweets = TRUE,
          writeToFile = TRUE,
          verbose = TRUE) # use 'verbose' to show download progress


# Clean the tweet text

clean_text <- twitter_data$tweets$text  %>% 
  rm_twitter_url() %>% 
  replace_url() %>% 
  replace_hash() %>% 
  replace_tag() %>% 
  replace_internet_slang() %>% 
  replace_emoji() %>% 
  replace_emoticon() %>% 
  replace_non_ascii() %>% 
  replace_contraction() %>% 
  gsub("[[:punct:]]", " ", .) %>% 
  gsub("[[:digit:]]", " ", .) %>% 
  gsub("[[:cntrl:]]", " ", .) %>% 
  gsub("\\s+", " ", .) %>% 
  tolower()


# Convert clean tweet vector into a document corpus (collection of documents)

text_corpus <- VCorpus(VectorSource(clean_text))

text_corpus[[1]]$content
text_corpus[[5]]$content


# Remove stop words

text_corpus <- text_corpus %>%
  tm_map(removeWords, stopwords(kind = "SMART")) 

text_corpus[[1]]$content
text_corpus[[5]]$content


# Transform corpus into a Document Term Matrix and remove 0 entries

doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]


# Optional: Remove objects and run garbage collection for faster processing

save(dtm, file = "doc_term_matrix_5_1.RData")
rm(list = ls(all.names = TRUE))
gc() 
load("doc_term_matrix_5_1.RData")


# Create LDA model with k topics

lda_model <- LDA(dtm, k = 6)


# Generate topic probabilities for each word
# 'beta' shows the probability that this word was generated by that topic

tweet_topics <- tidy(lda_model, matrix = "beta")

# Visualise the top 10 terms per topic

top_terms <- tweet_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
